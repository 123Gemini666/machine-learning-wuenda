---
title: 吴恩达笔记4
top: false
cover: false
toc: true
mathjax: true
copyright: true
date: 2019-11-27 23:29:01
password:
summary:
tags:
  - ML
  - 神经网络
categories:
  - Machine learning
  - 吴恩达
---



吴恩达笔记4

在本章节中主要讲解的是神经网络的基础知识：

- 非线性假设
- 神经元和大脑
- 模型表示
- 特征和直观理解
- 多类分类问题

<!--MORE-->

### 非线性假设Non-linear Hypotheses



线性回归和逻辑回归的缺点：**特征太多的时候，计算负荷会非常大**



假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车），我们怎样才能这么做呢？一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。

![QCQ1HA.png](https://s2.ax1x.com/2019/11/27/QCQ1HA.png)



![QCQcCV.png](https://s2.ax1x.com/2019/11/27/QCQcCV.png)



假设采用的是50*50像素的小图片，将所有的像素视为特征，则有2500个特征。普通的逻辑回归模型不能处理的，需要使用神经网络。



### 神经元和大脑

![QClMGV.png](https://s2.ax1x.com/2019/11/27/QClMGV.png)

### 模型表示

#### 模型表示1

每个神经元是可以被认为一个处理单元/神经核**processing unit**/**Nucleus**，主要包含：

- 多个输入/树突（**input**/**Dendrite**
- 一个输出/轴突（**output**/**Axon**）

> 神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络



![MxiT4P.png](https://s2.ax1x.com/2019/11/25/MxiT4P.png)



1. 神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型
2. 神经元称之为激活单元**activation unit**；在神经网络中，参数又可被成为权重（**weight**）
3. 类似神经元的神经网络

![MxkdoR.png](https://s2.ax1x.com/2019/11/25/MxkdoR.png)

#### 神经网络

下图是逻辑回归模型作为自身学习模型的神经元示例

![QC1BYq.png](https://s2.ax1x.com/2019/11/27/QC1BYq.png)



**类似神经元的神经网络结构**



![QC3J41.png](https://s2.ax1x.com/2019/11/27/QC3J41.png)

- $x_1,x_2,x_3$是输入单元，将原始数据输入给它们

- **几个比较基础的概念**

  - 输入层：数据节点所在的层
  - 网络层：输出$h_i$连同它的网络层参数$w,b$
  - 隐藏层：网络层中间的层
  - 输出层：最后一层
  - 偏置单元：bias unit，每层加上偏置单元



  上面模型的激活单元和输出分别表示为：

  ![QCYGqO.png](https://s2.ax1x.com/2019/11/27/QCYGqO.png)



  **三个激活单元：**
  $$
  a^{(2)}_1 = g(\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3)
  $$

  $$
  a^{(2)}_2 = g(\Theta^{(1)}_{20}x_0+\Theta^{(1)}_{21}x_1+\Theta^{(1)}_{22}x_2+\Theta^{(1)}_{23}x_3)
  $$

  $$
  a^{(2)}_3	= g(\Theta^{(1)}_{30}x_0+\Theta^{(1)}_{31}x_1+\Theta^{(1)}_{32}x_2+\Theta^{(1)}_{33}x_3)
  $$



  **输出的表达式为：**
  $$
  h_{\Theta}^{(x)} = g(\Theta^{(2)}_{10}a^{(2)}_0)+g(\Theta^{(2)}_{11}a^{(2)}_1)+g(\Theta^{(2)}_{12}a^{(2)}_2)+g(\Theta^{(2)}_{13}a^{(2)}_3)
  $$


  >将特征矩阵的每行（一个训练实例）喂给了神经网络，最终需要将整个训练集都喂给神经网络。这种从左到右计算的算法称之为：**前向传播法FORWARD PROPAGATION**



  #### 模型标记的记忆方法

- $a^{(j)}_i$表示的是第$j$层的第$i$个激活单元

- $\theta^{(j)}$代表从第$j$层映射到第$j+1$层的权重矩阵；例如：上图所示的神经网络中$\theta^{(1)}$的尺寸为 3*4。其尺寸具体表示为

  - 以第$j$ 层的激活单元数量为**行数**

  - 以第 $j+1$层的激活单元数+1为**列数**的矩阵



#### 模型表示2

**（FORWARD PROPAGATION** ) 相对于使用循环来编码，利用向量化的方法会使得计算更为简便。
$$
x=\begin{bmatrix}x_0\\ x_1\\ x_2\\x_3\\ \end{bmatrix}
$$

$$
z^{(2)}=\begin{bmatrix}z_1^{(2)}\\ z_2^{(2)}\\ z_3^{(2)}\\\end{bmatrix}
$$

其中
$$
z^{(2)}=\Theta^{(1)}x
$$
就是上面三个激活单元式子中的括号里面部分
$$
a^{(2)}=g(z^{(2)})
$$
将输入$x$看成是$a^{(1)}$，则
$$
z^{(2)}=\Theta^{(1)} a^{(1)}
$$

$$
a^{(2)}=g(z^{(2)})
$$

$$
z^{(3)}=\Theta^{(2)} a^{(2)}
$$

那么输出h可以表示为
$$
h_{\Theta}(x)=a^{(3)}=g(z^{(3)})
$$


![QC02nI.png](https://s2.ax1x.com/2019/11/27/QC02nI.png)



### 特征和直观理解

神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(**AND**)、逻辑或(**OR**)

#### 实现逻辑”与AND”

| x_1  | x_2  | h    |
| ---- | ---- | ---- |
| 0    | 0    | 0    |
| 0    | 1    | 0    |
| 1    | 0    | 0    |
| 1    | 1    | 1    |

![QCry3F.png](https://s2.ax1x.com/2019/11/27/QCry3F.png)

#### 实现逻辑"或OR"

| x_1  | x_2  | h    |
| ---- | ---- | ---- |
| 0    | 0    | 0    |
| 0    | 1    | 1    |
| 1    | 0    | 1    |
| 1    | 1    | 1    |

![QCsFDs.png](https://s2.ax1x.com/2019/11/27/QCsFDs.png)

#### 实现逻辑“非not”

![QC5cvt.png](https://s2.ax1x.com/2019/11/27/QC5cvt.png)



### 多类分类问题

当输出中不止有两中分类时，比如使用神经网络算法来识别路人、汽车、摩托车等。

![QCIpG9.png](https://s2.ax1x.com/2019/11/27/QCIpG9.png)

- 输入向量有3个维度，两个中间层
- 输出层有4个神经元表示4中分类，也就是每一个数据在输出层都会出现$[a,b,c,d]^T$，且$[a,b,c,d]$中仅有一个为1，表示当前类



### TF中解决办法

上述多类分类问题和TF中手写数字问题类似，解决办法如下：

- 将输出设置为$d_{out}$个输出节点的向量，$d_{out}$与类别数相同
- 让第$i \in [1,d_{out}]$个输出值表示当前样本属于类别i的概率P
- **如果属于第$i$类，索引为$i$的位置设置为1，其余为0！！！！**
- 下图中：对于所有猫的图片，数字编码是0，one-hot编码为[1,0,0,0]；其他类推

![QpgRT1.png](https://s2.ax1x.com/2019/11/27/QpgRT1.png)

3. 手写数字图片数据

总类别数是10，即输出节点总数值$d_{out}=10$，假设某个样本的类别是i，即图片中的数字是$i$，需要一个长度为10的向量$y$，索引号为$i$的位置设置为1，其余是0。

- 0的one-hot编码是[1,0,0,0,….]
- 1的one-hot编码是[0,1,0,0,….]
- 其余类推
