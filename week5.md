---
title: 吴恩达笔记5
top: false
cover: false
toc: true
mathjax: true
copyright: true
date: 2019-11-29 15:13:57
password:
summary:
tags:
  - Backpropagation Algorithm
  - 神经网络
categories:
  - Machine learning
  - 吴恩达
---



week5-神经网络学习Neural Networks Learning

### 神经网络代价函数

#### 参数解释

对几个参数的标记方法进行说明解释

- m：训练样本个数
- x，y：输入和输出信号
- L：代表神经网络层数
- $S_I$：每层的神经元个数
- $S_l$：表示输出神经元个数

<!--MORE-->

![QkEJc4.png](https://s2.ax1x.com/2019/11/29/QkEJc4.png)

#### 分类讨论

主要是两类：二分类和多类分类

二类分类：$S_L=0,y=0/1$；输出是一个实数

$K$类分类：$S_L=k,y_i=1$表示分到第$i$类的情况。输出是一个多维向量

#### 代价函数

逻辑斯蒂回归中的代价函数
$$
J(\theta)=-\frac{1}{m}\sum^m_{i=1}[y^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)})\log(1-h_\theta(x^{i}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j
$$
在逻辑斯蒂回归中，只有一个输出变量称之为标量`scalar`。

但是在神经网络中会有多个输出变量，$h_\theta(x)$是一个$K$维的向量。

假设函数$h_\theta(x) \in R^K;h_\theta(x)=i^{th}$ output；表示的是第$i$个输出，代价函数为：
$$
J(\Theta)=-\frac{1}{m}[\sum^m_{i=1}\sum^h_{k=1}[y^{(i)}_k\log(h_\Theta(x^{(i)}))_k+(1-y^{(i)}_k)\log(1-h_\Theta(x^{i}))_k]+\frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_l+1}_{j=1}(\Theta_{ji}^{(l)})^2
$$
解释说明：

1. 期望通过代价函数来观察算法预测的结果和真实情况的误差
2. 每行特征会有$K$个预测，利用循环对每行进行预测
3. 在$K$个预测中选择出可能性最高的那个，将其和实际的数据$y$进行比较
4. 正则化项是排除了每个偏置$\theta_0$之后，每层$\theta$矩阵的求和
5. 参数$j$（由$s_l+1$层的激活单元数决定）循环所有的行，$i$（由$s_l$层的激活单元数决定）循环所有的列



### 反向传播法Backpropagation Algorithm

![QkuzP1.png](https://s2.ax1x.com/2019/11/29/QkuzP1.png)

为了计算神经网络中代价函数的偏导数$\frac{\partial J(\Theta)}{\partial \Theta_{ij^{(l)}}}$，需要使用反向传播法

- 首先计算最后一层的误差
- 再一层层地反向求出各层的误差，直到倒数第二层

#### 前向传播栗子

假设有一个数据样本$(x^{(1)},y^{(1)})$，神经网络是4层的，其中$K=S_L=L=4$

![QklGIH.png](https://s2.ax1x.com/2019/11/29/QklGIH.png)

前向传播法就是通过一层层地按照神经网络的顺序从输入层到输出层计算下去。

#### 反向传播栗子

![Qk1h9A.png](https://s2.ax1x.com/2019/11/29/Qk1h9A.png)

1. 从最后一层的误差开始计算：误差=激活单元的预测$a^{(4)}$和实际值之间的$y^{(k)}$之间的误差；用$\delta$表示误差，**误差=模型预测值-真实值**

2. $$
   \delta^{(4)} = a^{(4)} -y
   $$

   前一层的误差

$$
\delta^{(3)} = (\Theta^{(3)})^T\theta^{(4)}*g^`(z^{(3)})
$$

其中$g^`(z^{(3)})$是$S$型函数的导数
$$
g^`(z^{(3)})=a^{(3)}*(1-a^{(3)})
$$

3. 再前一层的误差

$$
\delta^{(2)} = (\Theta^{(2)})^T\theta^{(3)}*g^`(z^{(2)})
$$

第一层是输入变量，不存在误差

4. 假设$\lambda=0$，如果不做正则化处理时

$$
\frac{\partial J(\Theta)}{\partial \Theta_{ij}^{l}}=a_j^{(l)}\theta_i^{(l+1)}
$$

上面的式子中各个上下标的含义：

$l$代表的是第几层

$j$代表的是计算层中的激活单元的下标

$i$代表的是误差单元的下标

#### 算法

![QkUlLR.png](https://s2.ax1x.com/2019/11/29/QkUlLR.png)

- 利用正向传播方法计算每层的激活单元
- 利用训练集的真实结果与神经网络的预测结果求出最后一层的误差
- 最后利用该误差运用反向传播法计算出直至第二层的所有误差。
- 在求出$\triangle ^{(l)}_{ij}$之后，便可以计算代价函数的偏导数$D^{(l)}_{ij}$



### 反向传播的直观理解

#### 前向传播原理

- 2个输入单元；2个隐藏层（不包含偏置单元）；1个输出单元
- 上标$i$表示的是第几层，下标表示的是第几个特征或者说属性

**图中有个小问题，看截图的右下角！！！**

![Qk0qER.png](https://s2.ax1x.com/2019/11/29/Qk0qER.png)

**结论**
$$
Z^{(3)}_{1}=\Theta_{10}^{(2)}*1+\Theta_{11}^{(2)}*a^{(2)}_1+\Theta_{12}^{(2)}*a^{(2)}_2
$$

#### 反向传播原理

![QkjDu8.png](https://s2.ax1x.com/2019/11/29/QkjDu8.png)

![Qkj5uT.png](https://s2.ax1x.com/2019/11/29/Qkj5uT.png)

### 参数展开

上面的式子中实现了怎么利用反向传播法计算代价函数的导数，在这里介绍怎么将参数从矩阵形式展开成向量形式

![QkHoHs.png](https://s2.ax1x.com/2019/11/29/QkHoHs.png)

![Qkb328.png](https://s2.ax1x.com/2019/11/29/Qkb328.png)

### 梯度检验

如何求解在某点的导数

![QkqNQO.png](https://s2.ax1x.com/2019/11/29/QkqNQO.png)

在代价函数中怎么对某个参数$\theta$求导

![Qkqq6U.png](https://s2.ax1x.com/2019/11/29/Qkqq6U.png)

### 神经网络小结

#### 首要工作

在构建神经网络的时候，首先考虑的是**如何选择网络结构**：多少层和每层多少个神经单元

- 第一层的单元数即我们训练集的特征数量。

- 最后一层的单元数是我们训练集的结果的类的数量。

- 如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。

#### 训练神经网络步骤

1. 参数的**随机初始化**
2. 利用**正向传播**方法计算所有的$h_{\theta}(x)$
3. 编写**计算代价函数** $J$的代码
4. 利用**反向传播**方法计算所有偏导数
5. 利用**数值检验**方法检验这些偏导数
6. 使用**优化算法来最小化代价函数**
